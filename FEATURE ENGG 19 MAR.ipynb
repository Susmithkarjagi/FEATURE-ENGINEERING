{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fbf5332-7dfd-4876-bdfd-6ac6901ea3f9",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING ASSIGNMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f944b1b9-eca6-4b7a-9342-5065f7e9f07d",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ae53e4-2dfb-479f-81e6-e3cccfdd4727",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as normalization, is a data preprocessing technique used to rescale numerical features within a specific range. It transforms the values of the features to a common scale, typically between 0 and 1. This scaling is achieved by subtracting the minimum value of the feature and then dividing it by the difference between the maximum and minimum values.\n",
    "\n",
    "Here's an example of how Min-Max scaling can be applied in Python using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0876be4-f8b0-46b1-8e65-d9aff0e0fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.66666667]\n",
      " [0.25       0.33333333]\n",
      " [0.5        0.        ]\n",
      " [0.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[5, 10], [2, 8], [3, 6], [1, 12]])\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b87d800-50b8-4f91-a183-9597cab97bd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12b4967-91fd-4b94-b62c-49796b87c510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "edc8119f-44c9-4504-b229-e78da679e0ec",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e180063b-8fe3-4dbb-b3b0-be558ed31da5",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as normalization or vector normalization, is a feature scaling method that rescales the values of a feature to have a unit norm. It transforms the feature vectors to have a length or magnitude of 1, while preserving their direction in the original feature space.\n",
    "\n",
    "Unlike Min-Max scaling, which scales the values within a specific range (e.g., 0 to 1), the Unit Vector technique focuses on the relative magnitudes of the feature vectors. It is particularly useful when the magnitude of the features is essential, but the actual values are less significant.\n",
    "\n",
    "Here's an example of how to apply Unit Vector scaling in Python using the scikit-learn library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4565f5a-a196-4f71-8d37-22b59b292949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4472136  0.89442719]\n",
      " [0.4472136  0.89442719]\n",
      " [0.4472136  0.89442719]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[1, 2], [2, 4], [3, 6]])\n",
    "\n",
    "# Create a Normalizer object\n",
    "scaler = Normalizer(norm='l2')\n",
    "\n",
    "# Transform the data\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf36a80-1eed-4bfd-ab4f-eccdefff005d",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with two features represented by the columns. We want to apply Unit Vector scaling to the data.\n",
    "\n",
    "The Normalizer class from scikit-learn is used for Unit Vector scaling. We create an instance of the scaler, scaler, and specify the norm parameter as 'l2' to perform L2 normalization, which scales the feature vectors to have a Euclidean length of 1.\n",
    "\n",
    "The transform method is then used to apply the scaling to the data. The resulting scaled_data will have the same number of rows as the original dataset, but each row will have a unit norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813302fa-88e8-487e-a65d-2d181bde30de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e08d5ab2-6a87-4932-a8ec-26b66b26b1f4",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a875a1db-eb8c-4cd2-80ba-3ff8f0199fc5",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction in data analysis and machine learning. It identifies the most significant patterns and relationships in a dataset by transforming the original features into a new set of uncorrelated variables called principal components.\n",
    "\n",
    "The main goal of PCA is to reduce the dimensionality of a dataset while retaining as much information as possible. It achieves this by projecting the data onto a lower-dimensional subspace that captures the maximum variance in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c81461ef-091d-4fbb-a596-3d659018f908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-7.79422863  0.        ]\n",
      " [-2.59807621  0.        ]\n",
      " [ 2.59807621  0.        ]\n",
      " [ 7.79422863 -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a PCA object with desired number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "print(transformed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f191ec0a-8401-439a-8ec7-8597e621906c",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with three features represented by the columns. We want to apply PCA to reduce the dimensionality of the data to two dimensions.\n",
    "\n",
    "The PCA class from scikit-learn is used to perform PCA. We create an instance of the PCA object, pca, and specify the number of components we want to keep (in this case, 2).\n",
    "\n",
    "The fit_transform method is then used to fit the PCA model on the data and transform it into the new lower-dimensional representation. The resulting transformed_data will have the same number of rows as the original dataset, but each row will have only two values corresponding to the two principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9934e4-09c0-4bca-b925-371f82ab49a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4110f8fb-c3f6-4ce2-a5f3-38c74a763382",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac8108-f6df-467c-a0ba-eaad86bef727",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) can be used for feature extraction, which involves selecting a subset of relevant features from a high-dimensional dataset. Feature extraction aims to reduce the dimensionality of the data by creating new features that capture the most important information or patterns.\n",
    "\n",
    "PCA achieves feature extraction by transforming the original features into a new set of uncorrelated variables called principal components. These principal components are linear combinations of the original features and are sorted in descending order of variance. The first few principal components capture the most significant patterns in the data.\n",
    "\n",
    "By selecting a subset of the top-ranked principal components, PCA effectively performs feature extraction. The selected principal components can serve as the new features, representing the most relevant information in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55ccc814-6d94-4f54-b864-79f70947513e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-5.19615242e+00  2.56395025e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 5.19615242e+00  2.56395025e-16]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# Create a PCA object with desired number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data\n",
    "extracted_features = pca.fit_transform(data)\n",
    "\n",
    "print(extracted_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849ce8c3-184a-4250-819f-bd6df527db23",
   "metadata": {},
   "source": [
    "In this example, we have a dataset with three original features represented by the columns. We want to perform feature extraction using PCA to reduce the dimensionality to two components.\n",
    "\n",
    "The PCA class from scikit-learn is used, and we create an instance of the PCA object, pca, specifying the desired number of components (in this case, 2).\n",
    "\n",
    "The fit_transform method is then used to fit the PCA model on the data and transform it into the new feature space. The resulting extracted_features will have the same number of rows as the original dataset, but each row will have only two values corresponding to the selected principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2a43fc-cf2a-4e96-9146-0b519c42c9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57a69326-7106-49bb-a522-44934e6a4d1a",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82721e5-62f6-4827-b34e-bb2739821ee9",
   "metadata": {},
   "source": [
    "To preprocess the data for building a recommendation system for a food delivery service, you can use Min-Max scaling to normalize the features such as price, rating, and delivery time. Min-Max scaling will ensure that all these features are on a common scale and have values within a specific range, typically between 0 and 1. Here's how you can use Min-Max scaling for each feature:\n",
    "\n",
    "Price: Suppose the price feature ranges from $5 to $20. Using Min-Max scaling, you can transform the price values to a range between 0 and 1. For example, if you have a price value of $10, the scaled value will be:\n",
    "\n",
    "Scaled Price = (Price - Min(Price)) / (Max(Price) - Min(Price))\n",
    "= (10 - 5) / (20 - 5)\n",
    "= 0.5\n",
    "\n",
    "Rating: If the rating feature ranges from 1 to 5, you can apply Min-Max scaling to transform the rating values to the range between 0 and 1. For instance, if you have a rating value of 4, the scaled value will be:\n",
    "\n",
    "Scaled Rating = (Rating - Min(Rating)) / (Max(Rating) - Min(Rating))\n",
    "= (4 - 1) / (5 - 1)\n",
    "= 0.75\n",
    "\n",
    "Delivery Time: Suppose the delivery time feature ranges from 30 minutes to 60 minutes. You can use Min-Max scaling to transform the delivery time values to the range between 0 and 1. For example, if you have a delivery time of 45 minutes, the scaled value will be:\n",
    "\n",
    "Scaled Delivery Time = (Delivery Time - Min(Delivery Time)) / (Max(Delivery Time) - Min(Delivery Time))\n",
    "= (45 - 30) / (60 - 30)\n",
    "= 0.5\n",
    "\n",
    "By applying Min-Max scaling to the price, rating, and delivery time features, all these features will be scaled to a common range between 0 and 1. This scaling ensures that no single feature dominates the others due to differences in their original scales. It allows for fair comparison and effective utilization of the features in the recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afff4c85-3662-4086-a60c-1aa78f69abb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4344e04-eab2-4cb6-958d-2af1c63a56db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ddd4d1c-cc4c-41d4-8b07-4f14bd9de61d",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4563f-065c-4f77-bcfb-bd2df8b9c615",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of a dataset containing many features, such as company financial data and market trends for predicting stock prices, PCA (Principal Component Analysis) can be used. PCA helps identify the most significant patterns and relationships in the dataset by transforming the original features into a new set of uncorrelated variables called principal components.\n",
    "\n",
    "Here's how you can use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "Data Preprocessing: Begin by preprocessing the dataset, which may involve steps like handling missing values, normalization, and feature scaling. Ensure that the dataset is in a suitable format for PCA.\n",
    "\n",
    "Feature Standardization: Since PCA is sensitive to the scale of the features, it is essential to standardize the features. This involves subtracting the mean of each feature and dividing by its standard deviation, ensuring that all features have zero mean and unit variance.\n",
    "\n",
    "Apply PCA: Use a PCA implementation (e.g., scikit-learn's PCA) to perform dimensionality reduction. Specify the desired number of principal components you want to retain based on the trade-off between dimensionality reduction and retaining sufficient information.\n",
    "\n",
    "Fit PCA: Fit the PCA model on the standardized dataset. The model analyzes the covariance structure among the features to determine the principal components.\n",
    "\n",
    "Explained Variance Ratio: Examine the explained variance ratio, which indicates the amount of variance explained by each principal component. This helps assess the contribution of each component and decide how many components to retain.\n",
    "\n",
    "Dimensionality Reduction: Select the desired number of principal components based on the explained variance ratio. Retain the components that capture the majority of the variance in the data while discarding less significant components.\n",
    "\n",
    "Transform Data: Transform the original dataset into the lower-dimensional subspace by applying the dimensionality reduction technique determined in the previous step. This creates a new dataset consisting of the selected principal components.\n",
    "\n",
    "Model Training: Use the reduced-dimensional dataset to train your stock price prediction model. The reduced features will help reduce noise, focus on the most important patterns, and potentially improve the model's performance.\n",
    "\n",
    "By applying PCA, you can reduce the dimensionality of the dataset while retaining the most significant information captured by the principal components. This reduces computational complexity, helps avoid overfitting, and can improve the interpretability and generalization of your stock price prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d147912c-414c-4ed2-ba9e-59f0f69ebb24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce1bb600-77e3-4857-aebc-5e4aadef4dbc",
   "metadata": {},
   "source": [
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd11f02d-1091-493c-86e0-aff89a6f21d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.        ]\n",
      " [-0.57894737]\n",
      " [-0.05263158]\n",
      " [ 0.47368421]\n",
      " [ 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)\n",
    "\n",
    "# Create a MinMaxScaler object with desired feature range\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1887eb-813a-4303-b6bf-edc6f208f63f",
   "metadata": {},
   "source": [
    "In this example, we have the original dataset stored in the data array. We reshape the array to be a column vector to match the expected input format of the MinMaxScaler.\n",
    "\n",
    "We then create an instance of the MinMaxScaler class, scaler, and specify the desired feature range as (-1, 1).\n",
    "\n",
    "Next, we fit the scaler on the data and simultaneously transform it using the fit_transform method. The resulting scaled_data will contain the Min-Max scaled values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd1658f-a728-47f1-8655-e971a09aaa79",
   "metadata": {},
   "source": [
    "The values in the scaled_data array have been transformed to the range of -1 to 1 using Min-Max scaling. The value 1 in the original dataset corresponds to 1 in the scaled dataset, and the value 20 in the original dataset corresponds to -1 in the scaled dataset, while the other values are scaled proportionally between these bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3546c-5cd2-425b-a5fc-c396c7f62424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72ddea07-d367-48d8-9016-eba76dc9c20b",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c9ba8-752f-433c-870b-d334688d8344",
   "metadata": {},
   "source": [
    "To perform feature extraction using PCA on a dataset with features like height, weight, age, gender, and blood pressure, the number of principal components to retain would depend on the desired balance between dimensionality reduction and the amount of information preserved. Here are the steps to determine the number of principal components to retain:\n",
    "\n",
    "Data Preprocessing: Begin by preprocessing the dataset, which may involve steps like handling missing values, normalization, and feature scaling. Ensure that the dataset is in a suitable format for PCA.\n",
    "\n",
    "Standardize Features: Since PCA is sensitive to the scale of the features, it is essential to standardize the features. This involves subtracting the mean of each feature and dividing by its standard deviation, ensuring that all features have zero mean and unit variance.\n",
    "\n",
    "Apply PCA: Use a PCA implementation (e.g., scikit-learn's PCA) to perform dimensionality reduction. Apply PCA on the standardized dataset.\n",
    "\n",
    "Explained Variance Ratio: After applying PCA, analyze the explained variance ratio. The explained variance ratio indicates the proportion of variance explained by each principal component. It helps understand how much information is preserved by each component.\n",
    "\n",
    "Scree Plot: Plot the cumulative explained variance ratio as a function of the number of principal components. This plot provides insights into how many principal components are needed to retain a certain percentage of the total variance.\n",
    "\n",
    "Elbow Method: Look for an \"elbow\" in the scree plot, which signifies a significant drop in the explained variance ratio. The number of principal components at this elbow point is a common heuristic for selecting the number of components to retain.\n",
    "\n",
    "Retain Principal Components: Based on the scree plot, cumulative explained variance ratio, and elbow method, choose the number of principal components that explain a sufficient amount of variance while still reducing the dimensionality of the dataset. This choice is often a trade-off between dimensionality reduction and the amount of information retained.\n",
    "\n",
    "Transform Data: Transform the original dataset into the lower-dimensional subspace using the selected number of principal components.\n",
    "\n",
    "The optimal number of principal components to retain can vary depending on the dataset and the specific requirements of the application. It is recommended to analyze the explained variance ratio, scree plot, and apply domain knowledge to make an informed decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef51a02-5c55-48a8-bc36-a2fd2f6ff3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
